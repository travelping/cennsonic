#!/bin/bash

set -e

# shellcheck source=src/kube-utils
source "$(dirname "$0")/kube-utils"

POD_SUBNET="10.233.64.0/18"
SERVICE_SUBNET="10.233.0.0/18"

Usage=$(cat <<EOF
Usage: kube-node <Command>

Commands:
    master init <Node IP> <API IP> [Options]
    master join <Node IP> <API IP> <Join Info>
    master join-info
    master delete [Node Name]
    master upgrade
    master pki make <PKI Archive>
    master pki install <PKI Archive>

    worker join <Node IP> <API IP> <Join Info>
    worker delete <Node Name>

    restart
    reset

    role set <Role> [Node Name]
    role unset <Role> [Node Name]

Options (master init):
    --vrrp-iface=<Interface>  Deploy Keepalived and use this interface for VRRP
    --pod-subnet=<CIDR>       Pod network CIDR (default: $POD_SUBNET)
    --service-subnet=<CIDR>   Service network CIDR (default: $SERVICE_SUBNET)
EOF
)

function main {
    local StartTime; StartTime="$(start_time)"

    case "$1 $2 $3 $4 $5" in
        "master init $3 $4 $5")
            if [ "$#" -lt 4 ]; then stop "${Usage}"; fi

            local NodeIp="$3"
            local ApiIp="$4"

            shift 4
            master_init "${NodeIp}" "${ApiIp}" "$@"
            show_completion_time "${StartTime}"
        ;;
        "master join $3 $4 $5")
            case "$(k8s_version)" in
                v1.12.*) if [ "$#" -lt 4 ]; then stop "${Usage}"; fi ;;
                v1.13.*|\
                v1.14.*) if [ "$#" -ne 9 ]; then stop "${Usage}"; fi ;;
            esac

            local NodeIp="$3"
            local ApiIp="$4"

            shift 4
            master_join "${NodeIp}" "${ApiIp}" "$@"
            show_completion_time "${StartTime}"
        ;;
        "master join-info   ")
            master_join_info
        ;;
        "master delete $3  ")
            local NodeName="$3"

            master_delete "${NodeName}"
            show_completion_time "${StartTime}"
        ;;
        "master upgrade   ")
            master_upgrade
            show_completion_time "${StartTime}"
        ;;
        "master pki make $4 "|\
        "master pki install $4 ")
            if [ "$#" -ne 4 ]; then stop "${Usage}"; fi

            local PkiAction="$3"
            local PkiArchive="$4"

            "master_pki_${PkiAction}" "${PkiArchive}"
            show_completion_time "${StartTime}"
        ;;
        "worker join $3 $4 $5")
            if [ "$#" -ne 9 ]; then stop "${Usage}"; fi

            local NodeIp="$3"
            local ApiIp="$4"

            shift 4
            worker_join "${NodeIp}" "${ApiIp}" "$@"
            show_completion_time "${StartTime}"
        ;;
        "worker delete $3  ")
            if [ "$#" -ne 3 ]; then stop "${Usage}"; fi

            local NodeName="$3"

            worker_delete "${NodeName}"
            show_completion_time "${StartTime}"
        ;;
        "restart    ")
            node_restart
            show_completion_time "${StartTime}"
        ;;
        "reset    ")
            node_reset
            show_completion_time "${StartTime}"
        ;;
        "role set $3 $4 ")
            if [ "$#" -lt 3 ]; then stop "${Usage}"; fi

            local Role="$3"
            local NodeName="$4"

            node_role set "${Role}" "${NodeName}"
        ;;
        "role unset $3 $4 ")
            if [ "$#" -lt 3 ]; then stop "${Usage}"; fi

            local Role="$3"
            local NodeName="$4"

            node_role unset "${Role}" "${NodeName}"
        ;;
        *) stop "${Usage}" ;;
    esac
}

function master_init {
    local NodeIp="$1"
    local ApiIp="$2"
    local VrrpIface=""
    local PodSubnet="${POD_SUBNET}"
    local ServiceSubnet="${SERVICE_SUBNET}"

    shift 2
    local Arg
    for Arg in "$@"; do
        case "${Arg}" in
            --vrrp-iface=*) VrrpIface="${Arg#*=}"; shift ;;
            --pod-subnet=*) PodSubnet="${Arg#*=}"; shift ;;
            --service-subnet=*) ServiceSubnet="${Arg#*=}"; shift ;;
            *) stop unknown-option "${Arg}" ;;
        esac
    done

    log "Initialising first master of $(cluster_name)..."
    if node_is_active; then show_node_is_active; return; fi

    sudo ln -s "$(kubeconfig_master)" "$(kubeconfig)"

    resolve_node_hostname_to "${NodeIp}"
    resolve_kubernetes_to "${NodeIp}"

    kubelet_extra_set "$(kubelet_extra_master "${NodeIp}")"
    master_prepare new "${NodeIp}" "${PodSubnet}" "${ServiceSubnet}"

    kubeadm init --skip-token-print
    node_role set master
    install_calico "${NodeIp}" "${PodSubnet}"

    wait_for_core_pods

    if [ -n "${VrrpIface}" ]; then
        install_kubealived "${ApiIp}" "${VrrpIface}"
    fi

    log "Installing Multus CNI..."
    kubectl create --filename "$(templates)/multus-cni-config.yaml"
    kubectl create --filename "$(templates)/multus-cni-rbac.yaml"
    kubectl create --filename "$(templates)/multus-cni.yaml"

    log "Installing VXLAN Controller..."
    kubectl create --filename "$(templates)/kube-vxlan-controller.yaml"
}
function master_join {
    local NodeIp="$1"
    local ApiIp="$2"
    shift 2

    log "Joining master to $(cluster_name)..."
    if node_is_active; then show_node_is_active; return; fi

    sudo ln -s "$(kubeconfig_master)" "$(kubeconfig)"

    resolve_node_hostname_to "${NodeIp}"
    resolve_kubernetes_to "${ApiIp}"

    local NodeName; NodeName="$(node_name)"
    local PodSubnet; PodSubnet="$(pod_subnet not "${NodeName}")"
    local ServiceSubnet; ServiceSubnet="$(service_subnet not "${NodeName}")"

    kubelet_extra_set "$(kubelet_extra_master "${NodeIp}")"
    master_prepare existing "${NodeIp}" "${PodSubnet}" "${ServiceSubnet}"

    case "$(k8s_version)" in
        v1.12.*)
            local EtcdHostname
            local EtcdIp

            kubeadm_alpha certs all
            kubeadm_alpha kubelet config write-to-disk
            kubeadm_alpha kubelet write-env-file
            kubeadm_alpha kubeconfig kubelet
            sudo systemctl start kubelet

            EtcdHostname="$(etcd_hostname not "$(node_name)")"
            EtcdIp="$(etcd_ip not "$(node_name)")"

            log "Joining etcd ${EtcdHostname} (${EtcdIp})..."
            etcd_exec "${EtcdHostname}" "${EtcdIp}" \
                      member add "$(node_hostname)" "https://${NodeIp}:2380"

            kubeadm_alpha etcd local
            kubeadm_alpha kubeconfig all
            kubeadm_alpha controlplane all
            kubeadm_alpha kubelet config annotate-cri
            kubeadm_alpha mark-master
        ;;
        v1.13.*|\
        v1.14.*)
            sudo /opt/bin/kubeadm join \
                --apiserver-advertise-address "${NodeIp}" \
                --experimental-control-plane "$@"
    esac

    node_role set master
    wait_for_core_pods
    update_cluster_status
    calico_etcd_endpoints_add "${NodeIp}"
    ensure_dns_ha
    resolve_kubernetes_to "${NodeIp}"
}
function master_prepare {
    local ClusterState="$1"
    local NodeIp="$2"
    local PodSubnet="$3"
    local ServiceSubnet="$4"
    local ClusterDns="${ServiceSubnet%.*}.10"

    log "Preparing master for the ${ClusterState} $(cluster_name)..."

    local EtcdState="${ClusterState}"
    [ "${ClusterState}" = "existing" ] && EtcdCluster="$(etcd_cluster)"

    sudo sed -e "s/_K8S_VERSION_/$(k8s_version)/g" \
             -e "s/_API_HOSTNAME_/$(api_hostname)/g" \
             -e "s/_NODE_HOSTNAME_/$(node_hostname)/g" \
             -e "s/_NODE_IP_/${NodeIp}/g" \
             -e "s\\_ETCD_CLUSTER_\\${EtcdCluster}\\g" \
             -e "s/_ETCD_STATE_/${EtcdState}/g" \
             -e "s\\_POD_SUBNET_\\${PodSubnet}\\g" \
             -e "s\\_SERVICE_SUBNET_\\${ServiceSubnet}\\g" \
             -e "s/_CLUSTER_DNS_/${ClusterDns}/g" \
             "$(kubeadm_config_template)" | \
         sudo tee "$(kubeadm_config)" > /dev/null
    sudo chmod 600 "$(kubeadm_config)"
}
function master_join_info {
    local Token; Token="$(join_token)"
    Token="${Token:-$(join_token_create)}"

    printf %b "--token ${Token} " \
              "$(api_server_ip):$(api_server_port) " \
              "--discovery-token-ca-cert-hash " \
                  "sha256:$(discovery_token_ca_cert_hash)"
}
function master_delete {
    local NodeName; NodeName="${1:-$(node_name)}"
    local Node; Node="${NodeName}.$(cluster_name)"
    local ApiIp; ApiIp="$(api_server_ip not "${NodeName}")"

    log "Deleting ${NodeName} from $(cluster_name)..."
    node_role unset master "${NodeName}"

    kubectl drain "${Node}" --delete-local-data --ignore-daemonsets --force

    if [ -n "${ApiIp}" ]; then
        if [ "${NodeName}" = "$(node_name)" ]; then
            resolve_kubernetes_to "$(api_server_ip not "${NodeName}")"
        fi

        calico_etcd_endpoints_delete "$(etcd_ip "${NodeName}")"

        log "Retrieving ${NodeName} etcd ID..."

        local EtcdHostname; EtcdHostname="$(etcd_hostname not "${NodeName}")"
        local EtcdIp; EtcdIp="$(etcd_ip not "${NodeName}")"

        local EtcdId; EtcdId=$(etcd_exec "${EtcdHostname}" "${EtcdIp}" \
                                 member list | grep "${Node}" | sed s/:.*//)

        log "Removing etcd member ${NodeName} (id: ${EtcdId})..."
        etcd_exec "${EtcdHostname}" "${EtcdIp}" member remove "${EtcdId}"

        kubectl delete node "${Node}"
        update_cluster_status
    fi

    if [ "${NodeName}" = "$(node_name)" ]; then
        node_reset
    fi
}
function master_upgrade {
    local K8sVersion; K8sVersion="$(k8s_version)"

    log "Upgrading master of $(cluster_name) to ${K8sVersion}..."

    update_cluster_status
    sudo /opt/bin/kubeadm upgrade apply "${K8sVersion}" -y
    node_restart
}
function master_pki_make {
    local Archive="$1"

    log "Making PKI archive ${Archive}..."

    mkdir -p kubernetes/pki/etcd
    sudo cp "$(pki)"/{ca.{crt,key},sa.{key,pub}} kubernetes/pki
    sudo cp "$(pki)"/front-proxy-ca.{crt,key} kubernetes/pki
    sudo cp "$(pki)"/etcd/ca.{crt,key} kubernetes/pki/etcd
    sudo cp /etc/kubernetes/admin.conf kubernetes

    sudo chown -R "${USER}:${USER}" kubernetes
    tar zcf "${Archive}" kubernetes
    rm -rf kubernetes
}
function master_pki_install {
    local Archive="$1"

    log "Installing PKI from ${Archive}..."
    sudo tar xf "${Archive}" -C /etc
    sudo chown -R root:root /etc/kubernetes
    rm "${Archive}"
}

function worker_join {
    local NodeIp="$1"
    local ApiIp="$2"
    shift 2

    log "Joining worker to $(cluster_name)..."
    if node_is_active; then show_node_is_active; return; fi

    resolve_node_hostname_to "${NodeIp}"
    resolve_kubernetes_to "${ApiIp}"

    kubelet_extra_set "$(kubelet_extra_worker "${NodeIp}")"

    sudo mkdir -m 700 -p /etc/kubernetes/manifests
    sudo /opt/bin/kubeadm join "$@"

    sudo ln -s "$(kubeconfig_worker)" "$(kubeconfig)"

    case "$(k8s_version)" in
        v1.12.*) node_role set worker ;;
    esac
}
function worker_delete {
    local NodeName="$1"
    local Node; Node="${NodeName}.$(cluster_name)"

    echo "Deleting ${NodeName} from $(cluster_name)..."

    kubectl drain "${Node}" --delete-local-data --ignore-daemonsets --force
    kubectl delete node "${NodeName}.$(cluster_name)"
}

function node_is_active {
    systemctl is-active kubelet --quiet
}
function show_node_is_active {
    log "Kubelet is active. The node might have been already provisioned."
}

function node_restart {
    log "Restarting node of $(cluster_name)..."
    sudo systemctl restart kubelet
}

function node_reset {
    log "Resetting node of $(cluster_name)..."

    sudo /opt/bin/kubeadm reset --force
    sudo rm -f "$(kubeadm_config)"
    sudo rm -f "$(kubeconfig)"

    cni_reset
    resolve_reset
    kubelet_extra_reset
    iptables_reset
}

function node_role {
    local Action="$1"
    local Role="$2"
    local NodeName; NodeName="${3:-$(node_name)}"

    case "${Action} ${Role}" in
        "set worker") node_role_taint unset master "${NodeName}" ;;
        "unset worker") if node_has_role_label master "${NodeName}"; then
                            node_role_taint set master "${NodeName}"
                        fi ;;
        "set master") if ! node_has_role_label worker "${NodeName}"; then
                          node_role_taint set master "${NodeName}"
                      fi ;;
        *) node_role_taint "${Action}" "${Role}" "${NodeName}" ;;
    esac

    node_role_label "${Action}" "${Role}" "${NodeName}"
}
function node_role_label {
    local Action="$1"
    local Role="$2"
    local NodeName="$3"

    local Node; Node="${NodeName}.$(cluster_name)"
    local Label="node-role.kubernetes.io/${Role}"

    case "${Action}" in
          set) if ! node_has_role_label "${Role}" "${NodeName}"; then
                   kubectl label node "${Node}" "${Label}=" --overwrite
               fi ;;
        unset) if node_has_role_label "${Role}" "${NodeName}"; then
                   kubectl label node "${Node}" "${Label}-"
               fi ;;
    esac
}
function node_role_taint {
    local Action="$1"
    local Role="$2"
    local NodeName="$3"

    local Node; Node="${NodeName}.$(cluster_name)"
    local Taint="node-role.kubernetes.io/${Role}"

    case "${Action}" in
          set) if ! node_has_role_taint "${Role}" "${NodeName}"; then
                   kubectl taint node "${Node}" "${Taint}=:NoSchedule"
               fi ;;
        unset) if node_has_role_taint "${Role}" "${NodeName}"; then
                   kubectl taint node "${Node}" "${Taint}:NoSchedule-"
               fi ;;
    esac
}
function node_has_role_label {
    local Role="$1"
    local NodeName="$2"

    local Node; Node="${NodeName}.$(cluster_name)"
    local Label="node-role\\.kubernetes\\.io/${Role}"

    local JsonPath="{.items[?(@.metadata.labels.${Label}=='')].metadata.name}"
    [[ "$(kubectl get nodes -o "jsonpath=${JsonPath}")" =~ ${Node} ]]
}
function node_has_role_taint {
    local Role="$1"
    local NodeName="$2"

    local Node; Node="${NodeName}.$(cluster_name)"
    local Taint="node-role.kubernetes.io/${Role}"

    local JsonPath="jsonpath={.spec.taints[?(@.key==\"${Taint}\")].effect}"
    [ "$(kubectl get node "${Node}" -o "${JsonPath}")" = "NoSchedule" ]
}

function update_cluster_status {
    log "Updating cluster status in Kubeadm ConfigMap..."
    kubectl patch configmap kubeadm-config \
            --namespace kube-system \
            --patch "$(cluster_status_patch)"
}
function cluster_status_patch {
    printf "%b" \
        "data:\\n" \
        "  ClusterStatus: |\\n" \
        "    apiEndpoints:\\n" \
        "$(cluster_status_api_endpoints)\\n" \
        "    apiVersion: $(cluster_status_api_version)\\n" \
        "    kind: ClusterStatus\\n"
}
function cluster_status_api_version {
    kubectl get configmap kubeadm-config \
            --namespace kube-system \
            --output jsonpath="{.data.ClusterStatus}" \
        |
        sed -n "s/apiVersion: //p"
}
function cluster_status_api_endpoints {
    local MasterNodes
    MasterNodes="$(kubectl get nodes --output jsonpath="$(master_nodes_jp)")"

    local Node
    for Node in ${MasterNodes}; do
        local NodeName="${Node%%.*}"
        echo "      ${Node}:"
        echo "        advertiseAddress: $(api_server_ip "${NodeName}")"
        echo "        bindPort: $(api_server_port "${NodeName}")"
    done
}
function master_nodes_jp {
    printf "%b" "{.items[?(@.metadata.labels" \
                    ".node-role\\.kubernetes\\.io/master==''" \
                ")].metadata.name}"
}

function ip_ready {
    local Ip="$1"
    ping "${Ip}" -c1 -W1 > /dev/null 2>&1
}

function install_kubealived {
    local VrrpIp="$1"
    local VrrpIface="$2"

    log "Installing Kubealived..."
    sudo sed -e "s/_VRID_/$((1 + RANDOM % 255))/" \
             -e "s/_IFACE_/${VrrpIface}/" \
             -e "s/_IP_/${VrrpIp}/" \
             -e "s~_PASSWORD_~$(openssl rand -base64 8)~" \
             "$(templates)/kubealived.yaml" | \
         kubectl create --filename -

    log "Waiting for Kubealived..."
    wait_for kubealived_ready

    log "Waiting for VRRP IP..."
    wait_for ip_ready "${VrrpIp}"
}
function kubealived_ready {
    [ "$(pods "" kubealived-system app=kubealived \
              "{.items[0].status.containerStatuses[0].ready}" \
              2>/dev/null)" = "true" ]
}
#function kubealived_terminated {
#    local NodeName="$1"
#    local Node; Node="${NodeName}.$(cluster_name)"
#    local JsonPath="{.items[?(@.spec.nodeName==\"${Node}\")].status.phase}"
#    local Phase
#
#    Phase="$(pods "" kubealived-system "" "$JsonPath")" && [ -z "${Phase}" ]
#}

function etcd_exec {
    local EtcdHostname="$1"
    local EtcdIp="$2"

    shift 2
    kubectl --namespace kube-system \
            exec "etcd-${EtcdHostname}" -- \
        etcdctl --ca-file "$(pki)/etcd/ca.crt" \
                --key-file "$(pki)/etcd/peer.key" \
                --cert-file "$(pki)/etcd/peer.crt" \
                --endpoints="https://${EtcdIp}:2379" \
            "$@"
}
function etcd_cluster {
    local JsonPath; JsonPath=$(printf %b \
        '{range.items[*]}' \
            '{.spec.nodeName}{"=https://"}{.status.podIP}{":2380,"}' \
        '{end}')
    pods "" kube-system component=etcd "${JsonPath}"
}
function etcd_hostname {
    pods "" kube-system component=etcd \
         "{.items[$(node_index "$@")].spec.nodeName}" | sed "s/ .*//"
}
function etcd_ip {
    pods "" kube-system component=etcd \
         "{.items[$(node_index "$@")].status.podIP}" | sed "s/ .*//"
}

function service_subnet {
    pod_command_arg component=kube-apiserver service-cluster-ip-range "$@"
}
function pod_subnet {
    pod_command_arg component=kube-controller-manager cluster-cidr "$@"
}

function api_server_ip {
    pod_command_arg component=kube-apiserver advertise-address "$@"
}
function api_server_port {
    pod_command_arg component=kube-apiserver secure-port
}

function pod_command_arg {
    local Selector="$1"
    local Arg="$2"
    shift 2

    pods "" kube-system "${Selector}" "$(pod_command_jp "$@")" |
        sed -n "s/--${Arg}=//p" | head -n1
}
function pod_command_jp {
    printf "%b" "{range.items[$(node_index "$@")]" \
                    ".spec.containers[*].command[*]}" \
                "{@}{\"\\\\n\"}{end}"
}

function wait_for_core_pods {
    declare -A Labels; declare -a Apps

    Labels[kube-proxy]=k8s-app; Apps+=(kube-proxy)
    Labels[calico-node]=k8s-app; Apps+=(calico-node)
    Labels[kube-apiserver]=component; Apps+=(kube-apiserver)
    Labels[kube-scheduler]=component; Apps+=(kube-scheduler)
    Labels[kube-controller-manager]=component; Apps+=(kube-controller-manager)
    Labels[etcd]=component; Apps+=(etcd)

    local App
    for App in "${Apps[@]}"; do
        echo "Waiting for ${App}..."
        wait_for core_pod_ready "${Labels[${App}]}" "${App}"
    done
}
function core_pod_ready {
    local Label="$1"
    local App="$2"
    local JsonPath; JsonPath="$(printf "%b" \
        "{.items[?(@.spec.nodeName==\"$(node_hostname)\")]" \
         ".status.containerStatuses[0].ready}" \
    )"

    [ "$(pods "" kube-system "${Label}=${App}" "${JsonPath}" \
         2>/dev/null)" = "true" ]
}

function pods {
    local PodName="$1"
    local Namespace="$2"
    local Selector="$3"
    local JsonPath="$4"
    local Command="kubectl get pods"

    [ -n "${PodName}" ] && Command="${Command} ${PodName}"
    [ -n "${Namespace}" ] && Command="${Command} --namespace ${Namespace}"
    [ -n "${Selector}" ] && Command="${Command} --selector ${Selector}"
    [ -n "${JsonPath}" ] && Command="${Command} --output jsonpath=${JsonPath}"

    ${Command}
}

function cni_reset {
    log "Resetting CNI..."
    sudo find /opt/cni/bin -type f -exec rm -f "{}" 2>/dev/null \; || true
    sudo find /etc/cni/net.d -type f -exec rm -f "{}" 2>/dev/null \; || true
    sudo rmdir -p /opt/cni/bin 2>/dev/null || true
    sudo rmdir -p /etc/cni/net.d 2>/dev/null || true
}

function kubelet_extra_master {
    local NodeIp="$1"
    printf %b "--node-ip=${NodeIp} "
}
function kubelet_extra_worker {
    printf %b "--node-ip=${NodeIp} " \
              "--allowed-unsafe-sysctls=net.* " \
              "--volume-plugin-dir=/var/lib/kubelet/volume-plugins "
}
function kubelet_extra_set {
    local Args="$1"
    log "Setting Kubelet extra flags ${Args}..."; echo

    printf %b "KUBELET_EXTRA_ARGS=${Args}" | sudo tee /etc/default/kubelet
    sudo chmod 600 /etc/default/kubelet
}
function kubelet_extra_reset {
    log "Resetting Kubelet extra flags..."
    sudo rm -f /etc/default/kubelet
}

function iptables_reset {
    local Tables=(raw nat mangle filter)

    log "Resetting iptables..."

    local Table
    for Table in "${Tables[@]}"; do
        sudo iptables -t "${Table}" -F
        sudo iptables -t "${Table}" -X
    done
}

function ensure_dns_ha {
    log "Ensuring DNS pods are on different nodes..."

    local Nodes
    Nodes="$(pods "" kube-system k8s-app=kube-dns "{.items[*].spec.nodeName}")"

    if [ "${Nodes%% *}" == "${Nodes##* }" ]; then
        local Pod; Pod="$(pods "" kube-system k8s-app=kube-dns \
                               "{.items[0].metadata.name}")"
        kubectl delete pod "${Pod}" --namespace kube-system
    fi
    wait_for_dns
}
function wait_for_dns {
    log "Waiting for kube-dns..."
    wait_for dns_ready
}
function dns_ready {
    [ "$(pods "" kube-system k8s-app=kube-dns \
              "{.items[*].status.containerStatuses[*].ready}" \
         2>/dev/null)" = "true true" ]
}

function install_calico {
    local NodeIp="$1"
    local PodSubnet="$2"

    log "Installing Calico..."
#   kubectl create --filename "$(templates)/calico-kdd-rbac.yaml"
#   kubectl create --filename "$(templates)/calico-kdd.yaml"

    sudo sed -e "s~_POD_SUBNET_~${PodSubnet}~" \
             "$(templates)/calico-etcd-config.yaml" |
         kubectl create --filename -

    kubectl create --filename "$(templates)/calico-etcd-sync.yaml"

    calico_etcd_secrets_create
    calico_etcd_endpoints_add "${NodeIp}"

    kubectl create --filename "$(templates)/calico-etcd-rbac.yaml"

    sudo sed -e "s~_POD_SUBNET_~${PodSubnet}~" \
             "$(templates)/calico-etcd.yaml" |
         kubectl create --filename -
}
function calico_etcd_secrets_create {
    local Pki; Pki="$(pki)/calico"

    log "Creating certificates for Calico to access etcd..."

    sudo mkdir -p "${Pki}"
    sudo cp "$(pki)/etcd/ca.crt" "${Pki}/etcd-ca"

    sudo openssl genrsa -out "${Pki}/etcd-key" 2048

    sudo openssl req -new \
                     -key "${Pki}/etcd-key" \
                     -out "${Pki}/etcd-csr" \
                     -subj "/CN=calico.$(cluster_name)"

    printf "%b" \
           "[ext]\\n" \
           "basicConstraints = CA:false\\n" \
           "keyUsage = digitalSignature,keyEncipherment\\n" \
           "extendedKeyUsage = serverAuth,clientAuth\\n" \
           "subjectAltName = DNS:localhost,IP:127.0.0.1\\n" \
           "subjectKeyIdentifier = hash\\n" | \
        sudo tee "${Pki}/etcd-cnf" > /dev/null

    sudo openssl x509 -req \
                      -in "${Pki}/etcd-csr" \
                      -out "${Pki}/etcd-cert" \
                      -CA "$(pki)/etcd/ca.crt" \
                      -CAkey "$(pki)/etcd/ca.key" \
                      -CAcreateserial \
                      -days 3650 \
                      -extensions ext \
                      -extfile "${Pki}/etcd-cnf"

    kubectl create secret generic calico-etcd-secrets \
            --namespace kube-system \
            --from-file="${Pki}/etcd-ca" \
            --from-file="${Pki}/etcd-key" \
            --from-file="${Pki}/etcd-cert"

    sudo rm "${Pki}"/etcd-{ca,key,csr,cnf,cert}
    sudo rmdir "${Pki}"
}
function calico_etcd_endpoints_add {
    local Ip="$1"
    local Endpoint="https://${Ip}:2379"
    local Endpoints; Endpoints="$(calico_etcd_endpoints_list)"
    Endpoints="${Endpoints:-$Endpoint}"

    case "${Endpoints}" in
        *"${Ip}"*) ;;
        *) Endpoints="${Endpoints},${Endpoint}" ;;
    esac

    log "Adding Calico etcd endpoint ${Endpoint}..."
    calico_etcd_endpoints_set "${Endpoints}"
}
function calico_etcd_endpoints_delete {
    local Ip="$1"
    local Endpoint="https://${Ip}:2379"
    local Endpoints; Endpoints="$(calico_etcd_endpoints_list | \
                                      sed -e "s~${Endpoint}~~" \
                                          -e "s/^,//" -e "s/,,/,/" -e "s/,$//")"

    log "Deleting Calico etcd endpoint ${Endpoint}..."
    calico_etcd_endpoints_set "${Endpoints}"
}
function calico_etcd_endpoints_list {
    kubectl get configmap calico-config \
            --namespace kube-system \
            --output jsonpath="{.data.etcd_endpoints}"
}
function calico_etcd_endpoints_set {
    local Endpoints="$1"

    log "Setting Calico etcd endpoints to ${Endpoints}..."
    kubectl patch configmap calico-config \
            --namespace kube-system \
            --patch "$(calico_etcd_endpoints_patch "${Endpoints}")"
}
function calico_etcd_endpoints_patch {
    local Endpoints="$1"
    cat <<EOF
data:
  etcd_endpoints:
    ${Endpoints}
EOF
}

function join_token {
    sudo /opt/bin/kubeadm token list | grep -v "<invalid>" | sed -n "2s/ .*//p"
}
function join_token_create {
    sudo /opt/bin/kubeadm token create
}
function discovery_token_ca_cert_hash {
    openssl x509 -pubkey -in "$(pki)/ca.crt" | \
        openssl rsa -pubin -outform der 2>/dev/null | \
            openssl dgst -sha256 -hex | sed "s/^.* //"
}

function kubeadm {
    sudo /opt/bin/kubeadm --config "$(kubeadm_config)" "$@"
}
function kubeadm_alpha {
    kubeadm alpha phase "$@"
}
function kubeadm_config {
    echo "/etc/kubernetes/kubeadm.conf"
}
function kubeadm_config_template {
    case "$(k8s_version)" in
        v1.12.*) echo "$(templates)/kubeadm-v1alpha3.yaml" ;;
        v1.13.*|\
        v1.14.*) echo "$(templates)/kubeadm-v1beta1.yaml" ;;
    esac
}
function kubectl {
    sudo -H /opt/bin/kubectl --kubeconfig "$(kubeconfig)" "$@"
}
function kubeconfig {
    echo "/etc/kubernetes/kubeconfig"
}
function kubeconfig_master {
    echo "/etc/kubernetes/admin.conf"
}
function kubeconfig_worker {
    echo "/etc/kubernetes/kubelet.conf"
}

function pki {
    echo "/etc/kubernetes/pki"
}
function templates {
    echo "/etc/kubernetes/templates"
}

function resolve_node_hostname_to {
    local Ip="$1"
    ensure_in_hosts "${Ip}" "$(node_hostname)"
}
function resolve_kubernetes_to {
    local Ip="$1"
    ensure_in_hosts "${Ip}" "kubernetes"
}
function resolve_reset {
    delete_from_hosts "kubernetes"
}
function ensure_in_hosts {
    local Ip="$1"
    local HostName="$2"

    log "Ensuring ${Ip} ${HostName} is in /etc/hosts..."
    if grep -q "${HostName}" /etc/hosts; then
        sudo sed -i "s/.*\\(${HostName}\\).*/${Ip}    \\1/" /etc/hosts
    else
        sudo sed -i "$ a ${Ip}    ${HostName}" /etc/hosts
    fi
}
function delete_from_hosts {
    local HostName="$1"

    log "Deleting ${HostName} from /etc/hosts..."
    sudo sed -i "/${HostName}/d" /etc/hosts
}

main "$@"
